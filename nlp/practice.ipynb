{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello, I am Nikhil, This is NLP practical. \n",
    "There are many different techniques ! involved in this concept.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Nikhil, This is NLP practical.\n",
      "There are many different techniques !\n",
      "involved in this concept.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Nikhil',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'practical',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'different',\n",
       " 'techniques',\n",
       " '!',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'concept',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Nikhil',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'practical',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'different',\n",
       " 'techniques',\n",
       " '!',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'concept',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Nikhil',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'practical.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'different',\n",
       " 'techniques',\n",
       " '!',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'concept',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming and Its Types \n",
    "# Reduces a word to its word stem that affixes, suffixes and prefixes to the roots of the words known as lemma.\n",
    "words = ['eats', \"eaten\", \"eating\", \"wrote\", \"written\", \"writing\", \"dancer\", \"dancing\", \"woken\",\"awake\", \"waking\",\"programming\",\"programs\",\"finally\",\"finalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats---------->eat\n",
      "eaten---------->eaten\n",
      "eating---------->eat\n",
      "wrote---------->wrote\n",
      "written---------->written\n",
      "writing---------->write\n",
      "dancer---------->dancer\n",
      "dancing---------->danc\n",
      "woken---------->woken\n",
      "awake---------->awak\n",
      "waking---------->wake\n",
      "programming---------->program\n",
      "programs---------->program\n",
      "finally---------->final\n",
      "finalize---------->final\n"
     ]
    }
   ],
   "source": [
    "# Major disadvantage: Changes meaning of the word \n",
    "for word in words:\n",
    "    print(word +'---------->'+ ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$|en$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats---------->eat\n",
      "eaten---------->eat\n",
      "eating---------->eat\n",
      "wrote---------->wrot\n",
      "written---------->writt\n",
      "writing---------->writ\n",
      "dancer---------->dancer\n",
      "dancing---------->danc\n",
      "woken---------->wok\n",
      "awake---------->awak\n",
      "waking---------->wak\n",
      "programming---------->programm\n",
      "programs---------->program\n",
      "finally---------->finally\n",
      "finalize---------->finaliz\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word +'---------->'+ reg_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Snowball Stemmer\n",
    "# Performs better than porter stemmer \n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats------>eat\n",
      "eaten------>eaten\n",
      "eating------>eat\n",
      "wrote------>wrote\n",
      "written------>written\n",
      "writing------>write\n",
      "dancer------>dancer\n",
      "dancing------>danc\n",
      "woken------>woken\n",
      "awake------>awak\n",
      "waking------>wake\n",
      "programming------>program\n",
      "programs------>program\n",
      "finally------>final\n",
      "finalize------>final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'------>'+snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "# Overcomes the cons of stemmers \n",
    "# Wordnet Lemmatizer:output is known as lemma which is a root word rather than root stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "POS- Noun -n,\n",
    "Verb - v,\n",
    "adjective - a,\n",
    "adverb - r\n",
    "\"\"\"\n",
    "lemmatizer.lemmatize(\"writing\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats--->eat\n",
      "eaten--->eat\n",
      "eating--->eat\n",
      "wrote--->write\n",
      "written--->write\n",
      "writing--->write\n",
      "dancer--->dancer\n",
      "dancing--->dance\n",
      "woken--->wake\n",
      "awake--->awake\n",
      "waking--->wake\n",
      "programming--->program\n",
      "programs--->program\n",
      "finally--->finally\n",
      "finalize--->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'--->'+lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords, partofspeech, named entity recognition\n",
    "paragraph = \"\"\"This is our hope. This is the faith that I go back to the South with. With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\n",
    "\n",
    "This will be the day when all of God's children will be able to sing with new meaning: My country, 'tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrims' pride, from every mountainside, let freedom ring.\n",
    "\n",
    "And if America is to be a great nation, this must become true. And so let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that, let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\n",
    "\n",
    "And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. Free at last. Thank God almighty, we are free at last.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is our hope. This is the faith that I go back to the South with. With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\\n\\nThis will be the day when all of God's children will be able to sing with new meaning: My country, 'tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrims' pride, from every mountainside, let freedom ring.\\n\\nAnd if America is to be a great nation, this must become true. And so let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that, let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\\n\\nAnd when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. Free at last. Thank God almighty, we are free at last.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords and filter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hope', 'NN'), ('.', '.')]\n",
      "[('faith', 'NN'), ('go', 'VB'), ('back', 'RB'), ('south', 'RB'), ('.', '.')]\n",
      "[('faith', 'NN'), (',', ','), ('able', 'JJ'), ('hew', 'NN'), ('mountain', 'NN'), ('despair', 'NN'), ('stone', 'NN'), ('hope', 'NN'), ('.', '.')]\n",
      "[('faith', 'NN'), ('able', 'JJ'), ('transform', 'NN'), ('jangle', 'NN'), ('discord', 'NN'), ('nation', 'NN'), ('beautiful', 'JJ'), ('symphony', 'NN'), ('brotherhood', 'NN'), ('.', '.')]\n",
      "[('faith', 'NN'), ('able', 'JJ'), ('work', 'NN'), ('together', 'RB'), (',', ','), ('pray', 'NN'), ('together', 'RB'), (',', ','), ('struggle', 'NN'), ('together', 'RB'), (',', ','), ('go', 'VB'), ('jail', 'NN'), ('together', 'RB'), (',', ','), ('stand', 'VBP'), ('freedom', 'NN'), ('together', 'RB'), (',', ','), ('know', 'VBP'), ('free', 'JJ'), ('one', 'CD'), ('day', 'NN'), ('.', '.')]\n",
      "[('day', 'NN'), ('god', 'NN'), (\"'s\", 'POS'), ('children', 'NNS'), ('able', 'JJ'), ('sing', 'VBG'), ('new', 'JJ'), ('mean', 'NN'), (':', ':'), ('country', 'NN'), (',', ','), (\"'t\", \"''\"), ('thee', 'NN'), (',', ','), ('sweet', 'JJ'), ('land', 'NN'), ('liberty', 'NN'), (',', ','), ('thee', 'NN'), ('sing', 'NN'), ('.', '.')]\n",
      "[('land', 'NN'), ('father', 'NN'), ('die', 'NN'), (',', ','), ('land', 'NN'), ('pilgrims', 'NNS'), (\"'\", 'POS'), ('pride', 'NN'), (',', ','), ('every', 'DT'), ('mountainside', 'NN'), (',', ','), ('let', 'VB'), ('freedom', 'NN'), ('ring', 'NN'), ('.', '.')]\n",
      "[('america', 'RB'), ('great', 'JJ'), ('nation', 'NN'), (',', ','), ('must', 'MD'), ('become', 'VB'), ('true', 'JJ'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('prodigious', 'JJ'), ('hilltops', 'NNS'), ('new', 'JJ'), ('hampshire', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('mighty', 'NN'), ('mountains', 'VBZ'), ('new', 'JJ'), ('york', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('heighten', 'JJ'), ('alleghenies', 'NNS'), ('pennsylvania', 'VBP'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('snowcapped', 'JJ'), ('rockies', 'NNS'), ('colorado', 'VBP'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('curvaceous', 'JJ'), ('slop', 'NN'), ('california', 'NN'), ('.', '.')]\n",
      "[(',', ','), ('let', 'VB'), ('freedom', 'NN'), ('ring', 'VBG'), ('stone', 'NN'), ('mountain', 'NN'), ('georgia', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('lookout', 'NN'), ('mountain', 'NN'), ('tennessee', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('every', 'DT'), ('hill', 'NN'), ('molehill', 'VBZ'), ('mississippi', 'RBS'), ('.', '.')]\n",
      "[('every', 'DT'), ('mountainside', 'NN'), (',', ','), ('let', 'VB'), ('freedom', 'NN'), ('ring', 'NN'), ('.', '.')]\n",
      "[('happen', 'NN'), (',', ','), ('allow', 'JJ'), ('freedom', 'NN'), ('ring', 'NN'), (',', ','), ('let', 'VB'), ('ring', 'VBG'), ('every', 'DT'), ('village', 'NN'), ('every', 'DT'), ('hamlet', 'NN'), (',', ','), ('every', 'DT'), ('state', 'NN'), ('every', 'DT'), ('city', 'NN'), (',', ','), ('able', 'JJ'), ('speed', 'NN'), ('day', 'NN'), ('god', 'NN'), (\"'s\", 'POS'), ('children', 'NNS'), (',', ','), ('black', 'JJ'), ('men', 'NNS'), ('white', 'JJ'), ('men', 'NNS'), (',', ','), ('jews', 'NNS'), ('gentiles', 'NNS'), (',', ','), ('protestants', 'NNS'), ('catholics', 'NNS'), (',', ','), ('able', 'JJ'), ('join', 'NN'), ('hand', 'NN'), ('sing', 'VBG'), ('word', 'NN'), ('old', 'JJ'), ('negro', 'JJ'), ('spiritual', 'NN'), (':', ':'), ('free', 'JJ'), ('last', 'JJ'), ('.', '.')]\n",
      "[('free', 'JJ'), ('last', 'JJ'), ('.', '.')]\n",
      "[('thank', 'NN'), ('god', 'NN'), ('almighty', 'NN'), (',', ','), ('free', 'JJ'), ('last', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Apply stopwords and filter and then apply lemmatizer\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word.lower(), pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)\n",
    "    # Pos_Tag\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope .',\n",
       " 'faith go back south .',\n",
       " 'faith , able hew mountain despair stone hope .',\n",
       " 'faith able transform jangle discord nation beautiful symphony brotherhood .',\n",
       " 'faith able work together , pray together , struggle together , go jail together , stand freedom together , know free one day .',\n",
       " \"day god 's children able sing new mean : country , 't thee , sweet land liberty , thee sing .\",\n",
       " \"land father die , land pilgrims ' pride , every mountainside , let freedom ring .\",\n",
       " 'america great nation , must become true .',\n",
       " 'let freedom ring prodigious hilltops new hampshire .',\n",
       " 'let freedom ring mighty mountains new york .',\n",
       " 'let freedom ring heighten alleghenies pennsylvania .',\n",
       " 'let freedom ring snowcapped rockies colorado .',\n",
       " 'let freedom ring curvaceous slop california .',\n",
       " ', let freedom ring stone mountain georgia .',\n",
       " 'let freedom ring lookout mountain tennessee .',\n",
       " 'let freedom ring every hill molehill mississippi .',\n",
       " 'every mountainside , let freedom ring .',\n",
       " \"happen , allow freedom ring , let ring every village every hamlet , every state every city , able speed day god 's children , black men white men , jews gentiles , protestants catholics , able join hand sing word old negro spiritual : free last .\",\n",
       " 'free last .',\n",
       " 'thank god almighty , free last .']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Freedom is never dear at any price. It is the breath of life. What would a man not pay for living?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entity recognition\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
