{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello, I am Nikhil, This is NLP practical. \n",
    "There are many different techniques ! involved in this concept.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Nikhil, This is NLP practical.\n",
      "There are many different techniques !\n",
      "involved in this concept.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Nikhil',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'practical',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'different',\n",
       " 'techniques',\n",
       " '!',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'concept',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Nikhil',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'practical',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'different',\n",
       " 'techniques',\n",
       " '!',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'concept',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Nikhil',\n",
       " ',',\n",
       " 'This',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'practical.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'many',\n",
       " 'different',\n",
       " 'techniques',\n",
       " '!',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'this',\n",
       " 'concept',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming and Its Types \n",
    "# Reduces a word to its word stem that affixes, suffixes and prefixes to the roots of the words known as lemma.\n",
    "words = ['eats', \"eaten\", \"eating\", \"wrote\", \"written\", \"writing\", \"dancer\", \"dancing\", \"woken\",\"awake\", \"waking\",\"programming\",\"programs\",\"finally\",\"finalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats---------->eat\n",
      "eaten---------->eaten\n",
      "eating---------->eat\n",
      "wrote---------->wrote\n",
      "written---------->written\n",
      "writing---------->write\n",
      "dancer---------->dancer\n",
      "dancing---------->danc\n",
      "woken---------->woken\n",
      "awake---------->awak\n",
      "waking---------->wake\n",
      "programming---------->program\n",
      "programs---------->program\n",
      "finally---------->final\n",
      "finalize---------->final\n"
     ]
    }
   ],
   "source": [
    "# Major disadvantage: Changes meaning of the word \n",
    "for word in words:\n",
    "    print(word +'---------->'+ ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$|en$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats---------->eat\n",
      "eaten---------->eat\n",
      "eating---------->eat\n",
      "wrote---------->wrot\n",
      "written---------->writt\n",
      "writing---------->writ\n",
      "dancer---------->dancer\n",
      "dancing---------->danc\n",
      "woken---------->wok\n",
      "awake---------->awak\n",
      "waking---------->wak\n",
      "programming---------->programm\n",
      "programs---------->program\n",
      "finally---------->finally\n",
      "finalize---------->finaliz\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word +'---------->'+ reg_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Snowball Stemmer\n",
    "# Performs better than porter stemmer \n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats------>eat\n",
      "eaten------>eaten\n",
      "eating------>eat\n",
      "wrote------>wrote\n",
      "written------>written\n",
      "writing------>write\n",
      "dancer------>dancer\n",
      "dancing------>danc\n",
      "woken------>woken\n",
      "awake------>awak\n",
      "waking------>wake\n",
      "programming------>program\n",
      "programs------>program\n",
      "finally------>final\n",
      "finalize------>final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'------>'+snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization\n",
    "# Overcomes the cons of stemmers \n",
    "# Wordnet Lemmatizer:output is known as lemma which is a root word rather than root stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "POS- Noun -n,\n",
    "Verb - v,\n",
    "adjective - a,\n",
    "adverb - r\n",
    "\"\"\"\n",
    "lemmatizer.lemmatize(\"writing\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eats--->eat\n",
      "eaten--->eat\n",
      "eating--->eat\n",
      "wrote--->write\n",
      "written--->write\n",
      "writing--->write\n",
      "dancer--->dancer\n",
      "dancing--->dance\n",
      "woken--->wake\n",
      "awake--->awake\n",
      "waking--->wake\n",
      "programming--->program\n",
      "programs--->program\n",
      "finally--->finally\n",
      "finalize--->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'--->'+lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords, partofspeech, named entity recognition\n",
    "paragraph = \"\"\"This is our hope. This is the faith that I go back to the South with. With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\n",
    "\n",
    "This will be the day when all of God's children will be able to sing with new meaning: My country, 'tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrims' pride, from every mountainside, let freedom ring.\n",
    "\n",
    "And if America is to be a great nation, this must become true. And so let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that, let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\n",
    "\n",
    "And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. Free at last. Thank God almighty, we are free at last.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is our hope. This is the faith that I go back to the South with. With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\\n\\nThis will be the day when all of God's children will be able to sing with new meaning: My country, 'tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrims' pride, from every mountainside, let freedom ring.\\n\\nAnd if America is to be a great nation, this must become true. And so let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snowcapped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that, let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\\n\\nAnd when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. Free at last. Thank God almighty, we are free at last.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords and filter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hope', 'NN'), ('.', '.')]\n",
      "[('faith', 'NN'), ('go', 'VB'), ('back', 'RB'), ('south', 'RB'), ('.', '.')]\n",
      "[('faith', 'NN'), (',', ','), ('abl', 'JJ'), ('hew', 'NN'), ('mountain', 'NN'), ('despair', 'NN'), ('stone', 'NN'), ('hope', 'NN'), ('.', '.')]\n",
      "[('faith', 'NN'), ('abl', 'NN'), ('transform', 'NN'), ('jangl', 'NN'), ('discord', 'NN'), ('nation', 'NN'), ('beauti', 'NN'), ('symphoni', 'NN'), ('brotherhood', 'NN'), ('.', '.')]\n",
      "[('faith', 'NN'), ('abl', 'NN'), ('work', 'NN'), ('togeth', 'NN'), (',', ','), ('pray', 'NN'), ('togeth', 'NN'), (',', ','), ('struggl', 'NN'), ('togeth', 'NN'), (',', ','), ('go', 'VB'), ('jail', 'NN'), ('togeth', 'NN'), (',', ','), ('stand', 'VBP'), ('freedom', 'NN'), ('togeth', 'NNS'), (',', ','), ('know', 'VBP'), ('free', 'JJ'), ('one', 'CD'), ('day', 'NN'), ('.', '.')]\n",
      "[('day', 'NN'), ('god', 'NN'), (\"'s\", 'POS'), ('children', 'NNS'), ('abl', 'VBP'), ('sing', 'VBG'), ('new', 'JJ'), ('mean', 'NN'), (':', ':'), ('countri', 'NN'), (',', ','), (\"'t\", \"''\"), ('thee', 'NN'), (',', ','), ('sweet', 'JJ'), ('land', 'NN'), ('liberti', 'NN'), (',', ','), ('thee', 'NN'), ('sing', 'NN'), ('.', '.')]\n",
      "[('land', 'NN'), ('father', 'NN'), ('die', 'NN'), (',', ','), ('land', 'VBP'), ('pilgrim', 'NN'), (\"'\", \"''\"), ('pride', 'NN'), (',', ','), ('everi', 'JJ'), ('mountainsid', 'NN'), (',', ','), ('let', 'VB'), ('freedom', 'NN'), ('ring', 'NN'), ('.', '.')]\n",
      "[('america', 'RB'), ('great', 'JJ'), ('nation', 'NN'), (',', ','), ('must', 'MD'), ('becom', 'VB'), ('true', 'JJ'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('prodigi', 'NNS'), ('hilltop', 'JJ'), ('new', 'JJ'), ('hampshir', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('mighti', 'NNS'), ('mountain', 'VBP'), ('new', 'JJ'), ('york', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('heighten', 'JJ'), ('allegheni', 'JJ'), ('pennsylvania', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('snowcap', 'NN'), ('rocki', 'NN'), ('colorado', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('curvac', 'NNS'), ('slope', 'VBP'), ('california', 'NN'), ('.', '.')]\n",
      "[(',', ','), ('let', 'VB'), ('freedom', 'NN'), ('ring', 'VBG'), ('stone', 'NN'), ('mountain', 'NN'), ('georgia', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('lookout', 'NN'), ('mountain', 'NN'), ('tennesse', 'NN'), ('.', '.')]\n",
      "[('let', 'NN'), ('freedom', 'NN'), ('ring', 'VBG'), ('everi', 'NN'), ('hill', 'NN'), ('molehil', 'VBZ'), ('mississippi', 'NNS'), ('.', '.')]\n",
      "[('everi', 'NN'), ('mountainsid', 'NN'), (',', ','), ('let', 'VB'), ('freedom', 'NN'), ('ring', 'NN'), ('.', '.')]\n",
      "[('happen', 'NN'), (',', ','), ('allow', 'JJ'), ('freedom', 'NN'), ('ring', 'NN'), (',', ','), ('let', 'VB'), ('ring', 'VBG'), ('everi', 'JJ'), ('villag', 'NN'), ('everi', 'NN'), ('hamlet', 'NN'), (',', ','), ('everi', 'JJ'), ('state', 'NN'), ('everi', 'NN'), ('citi', 'NN'), (',', ','), ('abl', 'JJ'), ('speed', 'NN'), ('day', 'NN'), ('god', 'NN'), (\"'s\", 'POS'), ('children', 'NNS'), (',', ','), ('black', 'JJ'), ('men', 'NNS'), ('white', 'JJ'), ('men', 'NNS'), (',', ','), ('jew', 'NN'), ('gentil', 'NN'), (',', ','), ('protest', 'NN'), ('cathol', 'NN'), (',', ','), ('abl', 'JJ'), ('join', 'NN'), ('hand', 'NN'), ('sing', 'VBG'), ('word', 'NN'), ('old', 'JJ'), ('negro', 'JJ'), ('spiritu', 'NN'), (':', ':'), ('free', 'JJ'), ('last', 'JJ'), ('.', '.')]\n",
      "[('free', 'JJ'), ('last', 'JJ'), ('.', '.')]\n",
      "[('thank', 'NN'), ('god', 'NN'), ('almighti', 'NN'), (',', ','), ('free', 'JJ'), ('last', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Apply stopwords and filter and then apply lemmatizer\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word.lower(), pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = \" \".join(words)\n",
    "    # Pos_Tag\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope .',\n",
       " 'faith go back south .',\n",
       " 'faith , abl hew mountain despair stone hope .',\n",
       " 'faith abl transform jangl discord nation beauti symphoni brotherhood .',\n",
       " 'faith abl work togeth , pray togeth , struggl togeth , go jail togeth , stand freedom togeth , know free one day .',\n",
       " \"day god 's children abl sing new mean : countri , 't thee , sweet land liberti , thee sing .\",\n",
       " \"land father die , land pilgrim ' pride , everi mountainsid , let freedom ring .\",\n",
       " 'america great nation , must becom true .',\n",
       " 'let freedom ring prodigi hilltop new hampshir .',\n",
       " 'let freedom ring mighti mountain new york .',\n",
       " 'let freedom ring heighten allegheni pennsylvania .',\n",
       " 'let freedom ring snowcap rocki colorado .',\n",
       " 'let freedom ring curvac slope california .',\n",
       " ', let freedom ring stone mountain georgia .',\n",
       " 'let freedom ring lookout mountain tennesse .',\n",
       " 'let freedom ring everi hill molehil mississippi .',\n",
       " 'everi mountainsid , let freedom ring .',\n",
       " \"happen , allow freedom ring , let ring everi villag everi hamlet , everi state everi citi , abl speed day god 's children , black men white men , jew gentil , protest cathol , abl join hand sing word old negro spiritu : free last .\",\n",
       " 'free last .',\n",
       " 'thank god almighti , free last .']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Freedom is never dear at any price. It is the breath of life. What would a man not pay for living?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entity recognition\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorization\n",
    "# Bag of Words \n",
    "# Tf-Idf\n",
    "# N-grams\n",
    "# Bi-grams\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words \n",
    "# step 1: Filter the stopwords and lowercase the words \n",
    "# step 2: Build a vocabulary with all the unique words\n",
    "# step 3: Update count based on the frequency within the document \n",
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is our hope ',\n",
       " 'this is the faith that i go back to the south with ',\n",
       " 'with this faith we will be able to hew out of the mountain of despair a stone of hope ',\n",
       " 'with this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood ',\n",
       " 'with this faith we will be able to work together to pray together to struggle together to go to jail together to stand up for freedom together knowing that we will be free one day ',\n",
       " 'this will be the day when all of god s children will be able to sing with new meaning my country tis of thee sweet land of liberty of thee i sing ',\n",
       " 'land where my fathers died land of the pilgrims pride from every mountainside let freedom ring ',\n",
       " 'and if america is to be a great nation this must become true ',\n",
       " 'and so let freedom ring from the prodigious hilltops of new hampshire ',\n",
       " 'let freedom ring from the mighty mountains of new york ',\n",
       " 'let freedom ring from the heightening alleghenies of pennsylvania ',\n",
       " 'let freedom ring from the snowcapped rockies of colorado ',\n",
       " 'let freedom ring from the curvaceous slopes of california ',\n",
       " 'but not only that let freedom ring from stone mountain of georgia ',\n",
       " 'let freedom ring from lookout mountain of tennessee ',\n",
       " 'let freedom ring from every hill and molehill of mississippi ',\n",
       " 'from every mountainside let freedom ring ',\n",
       " 'and when this happens and when we allow freedom ring when we let it ring from every village and every hamlet from every state and every city we will be able to speed up that day when all of god s children black men and white men jews and gentiles protestants and catholics will be able to join hands and sing in the words of the old negro spiritual free at last ',\n",
       " 'free at last ',\n",
       " 'thank god almighty we are free at last ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        \n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams - are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document. They come into play when we deal with text data in NLP (Natural Language Processing) tasks. They have a wide range of applications, like language models, semantic features, spelling correction, machine translation, text mining, etc.\n",
    "from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(text,grams):  \n",
    "    model=[]\n",
    "    # model will contain n-gram strings\n",
    "    count=0\n",
    "    for token in text[:len(text)-grams+1]:  \n",
    "       model.append(text[count:count+grams])  \n",
    "       count=count+1  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'the', 'faith', 'that'],\n",
       " ['is', 'the', 'faith', 'that', 'i'],\n",
       " ['the', 'faith', 'that', 'i', 'go'],\n",
       " ['faith', 'that', 'i', 'go', 'back'],\n",
       " ['that', 'i', 'go', 'back', 'to'],\n",
       " ['i', 'go', 'back', 'to', 'the'],\n",
       " ['go', 'back', 'to', 'the', 'south'],\n",
       " ['back', 'to', 'the', 'south', 'with']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(dataset[1].split(), grams=5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x126 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 265 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = cv.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 109, 'is': 51, 'our': 81, 'hope': 47, 'the': 107, 'faith': 28, 'that': 106, 'go': 36, 'back': 9, 'to': 111, 'south': 95, 'with': 122, 'we': 117, 'will': 121, 'be': 10, 'able': 0, 'hew': 44, 'out': 82, 'of': 77, 'mountain': 68, 'despair': 24, 'stone': 100, 'transform': 113, 'jangling': 54, 'discords': 26, 'nation': 73, 'into': 50, 'beautiful': 11, 'symphony': 103, 'brotherhood': 14, 'work': 124, 'together': 112, 'pray': 85, 'struggle': 101, 'jail': 53, 'stand': 98, 'up': 115, 'for': 30, 'freedom': 32, 'knowing': 57, 'free': 31, 'one': 79, 'day': 23, 'when': 118, 'all': 1, 'god': 37, 'children': 18, 'sing': 91, 'new': 75, 'meaning': 63, 'my': 72, 'country': 21, 'tis': 110, 'thee': 108, 'sweet': 102, 'land': 58, 'liberty': 61, 'where': 119, 'fathers': 29, 'died': 25, 'pilgrims': 84, 'pride': 86, 'from': 33, 'every': 27, 'mountainside': 70, 'let': 60, 'ring': 89, 'and': 6, 'if': 48, 'america': 5, 'great': 38, 'must': 71, 'become': 12, 'true': 114, 'so': 94, 'prodigious': 87, 'hilltops': 46, 'hampshire': 40, 'mighty': 65, 'mountains': 69, 'york': 125, 'heightening': 43, 'alleghenies': 2, 'pennsylvania': 83, 'snowcapped': 93, 'rockies': 90, 'colorado': 20, 'curvaceous': 22, 'slopes': 92, 'california': 16, 'but': 15, 'not': 76, 'only': 80, 'georgia': 35, 'lookout': 62, 'tennessee': 104, 'hill': 45, 'molehill': 67, 'mississippi': 66, 'happens': 42, 'allow': 3, 'it': 52, 'village': 116, 'hamlet': 39, 'state': 99, 'city': 19, 'speed': 96, 'black': 13, 'men': 64, 'white': 120, 'jews': 55, 'gentiles': 34, 'protestants': 88, 'catholics': 17, 'join': 56, 'hands': 41, 'in': 49, 'words': 123, 'old': 78, 'negro': 74, 'spiritual': 97, 'at': 8, 'last': 59, 'thank': 105, 'almighty': 4, 'are': 7}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
